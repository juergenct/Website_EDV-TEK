<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Word Embeddings &amp; Topic Models – Projekt EDV-TEK</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./index.html" rel="prev">
<link href="./assets/logo/logo_tie_kompakt.svg" rel="icon" type="image/svg+xml">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-b53751a350365c71b6c909e95f209ed1.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-507fe7863f63d3d6be4f9010e8a10f52.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-d0413af163e925d132aa4696c82fc472.min.css" rel="prefetch" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="./index.html" class="navbar-brand navbar-brand-logo">
    <img src="./assets/logo/logo_tie.svg" alt="TIE Logo" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Projekt EDV-TEK</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:juergen.thiesen@tuhh.de"> <i class="bi bi-envelope" role="img" aria-label="Drop us a Mail">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-bi-github" role="link" data-bs-toggle="dropdown" aria-expanded="false">
      <i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-bi-github">    
        <li>
    <a class="dropdown-item" href="https://github.com/juergenct">
 <span class="dropdown-text">TIE Profile</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/juergenct/edv_tek">
 <span class="dropdown-text">Source Code</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./01_identifikation.html">Teil 1 - Identifikation von TEKs</a></li><li class="breadcrumb-item"><a href="./01_identifikation.html">Word Embeddings &amp; Topic Models</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Projekt EDV-TEK</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Teil 1 - Identifikation von TEKs</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01_identifikation.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Word Embeddings &amp; Topic Models</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Contents</h2>
   
  <ul class="collapse">
  <li><a href="#word-embeddings-topic-models" id="toc-word-embeddings-topic-models" class="nav-link active" data-scroll-target="#word-embeddings-topic-models">Word Embeddings &amp; Topic Models</a>
  <ul class="collapse">
  <li><a href="#motivation" id="toc-motivation" class="nav-link" data-scroll-target="#motivation">Motivation</a></li>
  <li><a href="#basics" id="toc-basics" class="nav-link" data-scroll-target="#basics">Basics</a></li>
  <li><a href="#preprocessing" id="toc-preprocessing" class="nav-link" data-scroll-target="#preprocessing">Preprocessing</a></li>
  <li><a href="#word-vectors" id="toc-word-vectors" class="nav-link" data-scroll-target="#word-vectors">Word Vectors</a></li>
  <li><a href="#topic-models" id="toc-topic-models" class="nav-link" data-scroll-target="#topic-models">Topic Models</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><ul class="collapse"><li><a href="https://github.com/juergenct/edv_tek/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./01_identifikation.html">Teil 1 - Identifikation von TEKs</a></li><li class="breadcrumb-item"><a href="./01_identifikation.html">Word Embeddings &amp; Topic Models</a></li></ol></nav></header>




<section id="word-embeddings-topic-models" class="level1">
<h1>Word Embeddings &amp; Topic Models</h1>
<iframe class="slide-deck" src="./slides/01_Slides_Embeddings.html" width="100%" height="500px"></iframe>
<section id="motivation" class="level2">
<h2 class="anchored" data-anchor-id="motivation">Motivation</h2>
<p>This course will focus on Natural Language Processing (NLP) in the context of Deep Learning (DL).</p>
<p>Deep Learning is generally referred to as a sub-area of Machine Learning. Deep Learning uses neural networks (sometimes referred to as deep neural networks) to model and analyze large datasets.</p>
<p>“Natural language processing is an area of research in computer science and artificial intelligence (AI) concerned with processing natural languages such as English or Mandarin. This processing generally involves translating natural language into data (numbers) that a computer can use to learn about the world. And this understanding of the world is sometimes used to generate natural language text that reflects that understanding.” (cite Natural Language Processing in Action)</p>
<p>In the field of Social Science we mostly deal with Natural Language. Hence, this course focusses on the intersection of those two topics.</p>
<p>The following table highlights some of the most common use cases of NLP. After finishing this course you should be able to tackle those tasks for your own projects.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 27%">
<col style="width: 72%">
</colgroup>
<thead>
<tr class="header">
<th>NLP Task</th>
<th>Definition</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Machine Translation</td>
<td>The process of automatically converting text from one language to another.</td>
</tr>
<tr class="even">
<td>Text Summarization</td>
<td>Summarzing large quantities of text by shortening the text or by extracting relevant topics and keyphrases.</td>
</tr>
<tr class="odd">
<td>Question Answering and Information Retrieval</td>
<td>The task of automatically providing answers to questions posed in natural language based on information retrieved from a large dataset or collection of documents.</td>
</tr>
<tr class="even">
<td>Chatbots and Dialogue Systems</td>
<td>Systems designed to simulate conversation with human users to assist, provide information, or entertain.</td>
</tr>
<tr class="odd">
<td>Speech Recognition and Text to Speech</td>
<td>Technologies that convert spoken language into text (Speech Recognition) and written text into spoken audio (Text to Speech).</td>
</tr>
<tr class="even">
<td>Annotating Linguistic Structure</td>
<td>The process of labeling textual data with syntactic, semantic, and morphological features of a language.</td>
</tr>
</tbody>
</table>
<p>(mostly taken from Speech and Language Processing)</p>
</section>
<section id="basics" class="level2">
<h2 class="anchored" data-anchor-id="basics">Basics</h2>
<p>This course will mainly focus on the processing of natural language to extract useful information. Most of the algorithms that we will introduce originate from the fields of statistics or geometry. The methodologies we will cover throughout this course will introduce you to the most common tasks in the overlap of NLP and DL.</p>
</section>
<section id="preprocessing" class="level2">
<h2 class="anchored" data-anchor-id="preprocessing">Preprocessing</h2>
<p>One key step in order for machines to work with natural language is preprocessing. In this part some of the most common preprocessing steps are briefly introduced.</p>
<section id="text-normalization" class="level3">
<h3 class="anchored" data-anchor-id="text-normalization">Text Normalization</h3>
<p>Generally the first step before applying any advanced NLP algorithms is dealing with Text Normalization The most important tasks that fall within this category are:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 27%">
<col style="width: 72%">
</colgroup>
<thead>
<tr class="header">
<th>Text Normalization Task</th>
<th>Definition</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Lowercasing</td>
<td>Normalizing the text by lowercasing all characters.</td>
</tr>
<tr class="even">
<td>Dealing with Regular Expressions*</td>
<td>Manipulating the text string by deleting, replacing certain patterns, symbols or characters.</td>
</tr>
<tr class="odd">
<td>(Stop)word Removal</td>
<td>Removing any irrelevant words from the text.</td>
</tr>
<tr class="even">
<td>Standardizing the text</td>
<td>Standardizing the text by encoding it to some well-known communication standards such as UTF-8 or ASCII.</td>
</tr>
</tbody>
</table>
<p>(mostly taken from Speech and Language Processing)</p>
<p>*Regular Expression refers to a specific language used for searching any patterns, symbols, characters within text strings.</p>
</section>
<section id="word-tokenization-and-chunking" class="level3">
<h3 class="anchored" data-anchor-id="word-tokenization-and-chunking">Word Tokenization and Chunking</h3>
<p>In NLP tokenization is used to break documents and sentences into smaller units. In most of the applications tokens are the words within a document delimited by spaces. Nonetheless for some applications, such as Topic Modelling, tokens can also be sequences of words generally referred to as n-grams. In particular, bigrams consist of two, trigrams of three words and so on. Chunking is a method to limit the input to the DL models depending generally to the allowed context window size. Tokenization is also a form of chunking. Both methods are necessary to deal with the limited context window of neural networks.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/img/tokenizer.jpg" class="img-fluid figure-img"></p>
<figcaption>Example for word tokenization</figcaption>
</figure>
</div>
</section>
<section id="stemming-and-lemmatization" class="level3">
<h3 class="anchored" data-anchor-id="stemming-and-lemmatization">Stemming and Lemmatization</h3>
<p>In some NLP applications it is important to reduce the multitude of forms words can occur in (e.g.&nbsp;to sing, singing, sang, etc. for sing) into a single normalized form.</p>
<p>A first approach to normalize those forms is called stemming. The goal of stemming is to eliminate pluralization, possesive ending of words and reduce various forms of verbs to their root form. Stemming removes suffixes from words without changing any characters. Hence, the words ‘to sing, singing and sang’ would be reduced to ‘to sing, sing and sang’.</p>
<p>The second very common normalization approach, called lemmatization, tries to capture the semantic root of a word by not only removing additional suffixes, but also for transforming words back to their root form (e.g.&nbsp;past tense verbs into present tense). For this lemmatizers rely on an extensive knowledge base of of word synonyms and sometimes even require grammatics to correctly assign the semantic root. In our example the words ‘to sing, singing and sang’ would be reduced to ‘to sing, sing and sing’.</p>
</section>
</section>
<section id="word-vectors" class="level2">
<h2 class="anchored" data-anchor-id="word-vectors">Word Vectors</h2>
<p>Word vectors or embeddings are a sophisticated form of feature representation that transforms words into vectors of real numbers. This transformation is crucial for machine learning models to process natural language data efficiently. Embeddings capture not only the semantic meanings of words but also their contextual relationships within a corpus, enabling models to perform tasks like sentiment analysis, topic modeling, and social network analysis effectively.</p>
<section id="one-hot-encoding" class="level3">
<h3 class="anchored" data-anchor-id="one-hot-encoding">One-Hot Encoding</h3>
<p>A first approach to construct vectors from words is the idea of one-hot encoding. The vectors dimension is equal to the number of words in the vocabulary for the underlying context.</p>
<p><em>Example:</em> - tree = [0 0 0 1 0] - bush = [0 0 1 0 0]</p>
<p>The biggest downside of this approach is that it is not able to capture any word relationships or semantics within this encoding. Even though the words tree and bush are closely related, their vectors are strictly orthogonal. Imagine a web search with the goal of extracting any passages related to forestry, due to the orthogonal nature of the vectors it would not be able to capture these two examples.</p>
</section>
<section id="semantical-word-representations" class="level3">
<h3 class="anchored" data-anchor-id="semantical-word-representations">Semantical Word Representations</h3>
<p>The solution is to embed the similarity of words in their vectoral representations.</p>
<p><em>Definition of Meaning</em> In NLP an often cited definition of meaning or semantics is the one given by J.R. Firth (1957 - search for reference) stating: “You shall know a word by the company it keeps”</p>
<p>This definition relates the semantics of a word towards other words which frequently appear in proximity to the focal word (distributional semantics). Instead of one-hot encoding words, their vector representations are <em>learned</em> which results in word embeddings having geometrical meaning (e.g.&nbsp;a scalar product of two similar words will be larger than the scalar product of two dissimilar words).</p>
<p><em>Example</em> - tree = [0.286 0.792 −0.177 −0.107 0.109] - bush = [0.295,0.785,−0.180,−0.115,0.112]</p>
<p>PUT HERE A NICE WORD EMBEDDING MAP WITH A FEW WORDS</p>
</section>
<section id="word2vec" class="level3">
<h3 class="anchored" data-anchor-id="word2vec">Word2Vec</h3>
<p>The framework Word2Vec, famously introduced by Mikolov et al.&nbsp;(2013), has been a groundbreaking paper implementing the notion of distributional semantics. Building upon a large body of text, the word vectors are iteratively adjusted based on their neighboring words. By relying on the maximization of the dot product between center and context word vectors, the word embeddings are constructed.</p>
<p>We will explain the example of the Word2Vec model in more detail, as it serves as one of the core foundations for working with word vectors. In general, there are two variants of the Word2Vec model:</p>
<ol type="1">
<li><em>Skip-Gram</em> - the word embeddings are fine-tuned based on the predictions of the context (outside) words given a center/focal word.</li>
<li><em>Continuous Bag of Words (CBOW)</em> - the word embeddings are fine-tuned based on the prediction of the center word, given the surrounding/context words.</li>
</ol>
<p>We will show the approach for coming up with the Skip-gram based word embeddings in more detail.</p>
<p>Let <em>c</em> denote the center word and <em>o</em> denote all context words. The overarching goal is to adjust the word embedding of <em>c</em> so that the probabilities shown in Figure (cite figure) are highest for the most prominent context words.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/img/word2vec.png" class="img-fluid figure-img"></p>
<figcaption>Source: https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1244/slides/cs224n-2024-lecture01-wordvecs1-public.pdf</figcaption>
</figure>
</div>
<p>The goal is to maximize the likelihood of predicting the correct context words, given the current feature vector of word <em>wt</em> at timestamp <em>t</em>.</p>
<p>Likelihood = <span class="math inline">\(L(\theta) = \prod_{t=1}^T \prod_{\substack{-m \leq j \leq m \\ j \neq 0}} P(w_{t+j} \mid w_t; \theta)\)</span></p>
<p><span class="math inline">\(\theta\)</span> denotes all parameters (in this case, word vectors in the vocabulary) of the model itself which are to be optimized. In the Word2Vec algorithm, each word in the vocabulary has two vectors, one for being the center word <em>vc</em> and one for being a context word <em>uo</em>.</p>
<p>In Machine Learning and Deep Learning, we prefer working with minimizing a so-called loss function. Therefore, we rewrite the expression as:</p>
<p><span class="math inline">\(J(\theta) = -\frac{1}{T} \log L(\theta) = -\frac{1}{T} \sum_{t=1}^T \sum_{\substack{-m \leq j \leq m \\ j \neq 0}} \log P(w_{t+j} \mid w_t; \theta)\)</span></p>
<p>This term is called the negative log likelihood and is a very common loss function in these fields.</p>
<p>So how do we calculate <span class="math inline">\(P(w_{t+j} \mid w_t; \theta)\)</span>? We simply take <em>vc</em> as the word vector of the center word and <em>uo</em> as the word vector of the context word and calculate the scalar product. The result is then normalized by the sum of all dot products of the word vector of the center word <em>vc</em> with all words from the vocabulary <em>uw</em>.</p>
<p><span class="math inline">\(P(o \mid c) = \frac{\exp(u_o^T v_c)}{\sum_{w \in V} \exp(u_w^T v_c)}\)</span></p>
<p>This expression is a common one in Deep Learning and Machine Learning called the Softmax function. This term is usually used in the context of neural networks to map arbitrary values (in this case, the scalar product of <em>uo</em> and <em>vc</em>) to a probability distribution, by amplifying (max) larger values in the distribution and still assigning soft values to smaller values in the distribution.</p>
<p>The model parameters, all feature vectors for all words in the vocabulary, are now gradually adjusted to decrease the negative log-likelihood.</p>
<p><span class="math inline">\(\theta = \begin{bmatrix}
v_{\text{aardvark}} \\
v_a \\
\vdots \\
v_{\text{zebra}} \\
u_{\text{aardvark}} \\
u_a \\
\vdots \\
u_{\text{zebra}}
\end{bmatrix} \in \mathbb{R}^{2dV}\)</span></p>
<p>Now, how do we adjust the parameters towards the best result? This is done by iteratively moving towards the direction of lower log-likelihood (negative gradient descent). For this, we need the derivative of the negative log-likelihood function, which in a high-dimensional vector space is called a gradient. The underlying concept is called gradient descent and is another prominent algorithm from the field of deep learning.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/img/gradientdescent.png" class="img-fluid figure-img"></p>
<figcaption>Source: https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1244/slides/cs224n-2024-lecture02-wordvecs2.pdf</figcaption>
</figure>
</div>
<p>So the equation for updating the parameters in our models becomes:</p>
<p><span class="math inline">\(\theta_{\text{new}} = \theta_{\text{old}} - \alpha \nabla_{\theta} J(\theta)\)</span></p>
<p>For large vocabularies, computing each and every derivative becomes very cumbersome. Therefore, in practice, the stochastic gradient descent (SGD) algorithm is very common. SGD randomly samples values from <span class="math inline">\(\theta\)</span> for which the derivatives are computed. By randomly choosing the values from <span class="math inline">\(\theta\)</span>, the algorithm converges.</p>
</section>
<section id="word-vector-characteristics" class="level3">
<h3 class="anchored" data-anchor-id="word-vector-characteristics">Word Vector Characteristics</h3>
<p>One of the most prominent analogies of word vector geometry is the capturing of semantic and syntactic analogies. Taking the word vectors of the words man, woman and king, one should be able to derive the word vector of queen.</p>
<p>In particular: <span class="math inline">\(man - woman = king - queen\)</span> -&gt; <span class="math inline">\(queen = king - man + woman\)</span></p>
<p>By harnessing the power of word embeddings we are able to compute word similarities. In this example the maxmium similarity is 10, whereas no similarity is indicated by 0. | Word1 | Word2 | Similarity | |———-|————|————| | vanish | disappear | 9.8 | | belief | impression | 5.95 | | muscle | bone | 3.65 | | modest | flexible | 0.98 | | hole | agreement | 0.3 | (taken from Speech and Language Processing)</p>
</section>
</section>
<section id="topic-models" class="level2">
<h2 class="anchored" data-anchor-id="topic-models">Topic Models</h2>
<p>We are now tackling the first real application of NLP within this course, which shall be Topic Models. Topic Models are used to generate or extract the n-grams which are most suitable to describe the content of a text. Therefore, these methodologies fall into the task of text summarization. Several different approaches shall be briefly introduced</p>
<section id="bag-of-words" class="level3">
<h3 class="anchored" data-anchor-id="bag-of-words">Bag-of-words</h3>
<p>A first approach is purely relying on word or token count and can be used to extract the most common tokens within a text and use them as their topics/keyphrases. In the following image (cite image) the NLP pipeline for the bag-of-words approach is nicely depictured. After tokenizing (and potentially lemmatizing or stemming) the tokens are used as input to the simple bag-of-words counter. Prior to counting the occurrences of each token the bag-of-words vector is initialized, including all relevant tokens whose frequencies shall be counted. Furthermore, rare tokens and stopwords are excluded from the analysis. <img src="assets/img/bag-of-words.png" class="img-fluid" alt="Source: Natural Language Processing in Action"></p>
</section>
<section id="tf-idf" class="level3">
<h3 class="anchored" data-anchor-id="tf-idf">Tf-Idf</h3>
<p>A more sophisticated and very common approach is called TF-Idf (Term Frequency - Inverse Document Frequency), which considers the term frequency of a word in one document and relates it to the inverse of the number of documents the term occurs in. The Tf-Idf score yields high values for terms which are very common within one document, but usually do not occur too frequently in the whole corpus. Hence, those words tend to be very important for one particular text.</p>
<p>The formulas for the TF-IDF are written as:</p>
<p><span class="math inline">\(tf(t, d) = \frac{\text{count}(t)}{\text{count}(d)}\)</span></p>
<p><span class="math inline">\(idf(t, D) = \log \left(\frac{\text{number of documents}}{\text{number of documents containing } t}\right)\)</span></p>
<p><span class="math inline">\(tfidf(t, d, D) = tf(t, d) \times idf(t, D)\)</span></p>
<p><span class="math inline">\(t\)</span> denotes the term for which we want to compute the Tf-Idf score, <span class="math inline">\(d\)</span> represents the focal document and <span class="math inline">\(D\)</span> the corpus of all documents. Therefore is <span class="math inline">\(count(t)\)</span> the count of term <span class="math inline">\(t\)</span> in document <span class="math inline">\(d\)</span> and <span class="math inline">\(count(d)\)</span> the count of all terms in document <span class="math inline">\(d\)</span> also including multiple occurences.</p>
</section>
<section id="latent-dirichlet-allocation-latent-dirichlet-allocation-by-david-m.-blei-andrew-y.-ng-and-michael-i.-jordan-httpwww.jmlr.orgpapersvolume3blei03ablei03a.pdf" class="level3">
<h3 class="anchored" data-anchor-id="latent-dirichlet-allocation-latent-dirichlet-allocation-by-david-m.-blei-andrew-y.-ng-and-michael-i.-jordan-httpwww.jmlr.orgpapersvolume3blei03ablei03a.pdf">Latent Dirichlet Allocation (“Latent Dirichlet Allocation” by David M. Blei, Andrew Y. Ng, and Michael I. Jordan (http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf))</h3>
<p>Latent Dirichlet Allocation (LDA) is a statistical model for topic modeling introduced by researchers David Blei, Andrew Ng, and Michael I. Jordan. It has become a foundational method in the field of natural language processing (NLP) for discovering abstract topics from a collection of documents.</p>
<p>LDA is based on the premise that documents are mixtures of topics, where a topic is characterized by a distribution/collection over words.</p>
<p>Modeling Process: Initialization: The user specifies the number of topics <span class="math inline">\(K\)</span> to be extracted from a corpus. Each topic is represented as a distribution over words, and each document is represented as a distribution over these topics.</p>
<p>Word Distribution: For each topic <span class="math inline">\(k, a\)</span> distribution over the vocabulary is sampled from a Dirichlet distribution defined by a parameter <span class="math inline">\(\beta\)</span>. Topic Distribution for Documents: For each document <span class="math inline">\(d\)</span>, a distribution over the topics is sampled from a Dirichlet distribution defined by a parameter <span class="math inline">\(\alpha\)</span>.</p>
<p>Word Assignments: Words in each document are assumed to be drawn from a topic-specific word distribution. The specific topic for each word is sampled according to the document’s distribution over topics.</p>
<p>Iterative Sampling: The assignments of topics to words are then iteratively updated through a process such as Gibbs sampling or variational inference. This iterative process continues until the model converges or a pre-specified number of iterations is reached.</p>
<p>The model yields clusters of words that represent topics. For example, a “finance” topic might include high probabilities for words like “bank,” “money,” and “debt.” Each document in the corpus is then represented as a mixture of these topics, which can be used for various applications such as document classification, information retrieval, and content recommendation.</p>
<p>LDA assumes documents are produced from hidden topics, where each topic is characterized by a distinct distribution over words. This allows it to capture hidden thematic patterns across documents, which are valuable for understanding and organizing large datasets.</p>
</section>
<section id="keybert-httpsmaartengr.github.iokeybert" class="level3">
<h3 class="anchored" data-anchor-id="keybert-httpsmaartengr.github.iokeybert">KeyBERT (https://maartengr.github.io/KeyBERT/)</h3>
<p>For the last two topics models let’s focus more on using Large Language Models for Topic Modelling.</p>
<p>The first approach is a very simple implementation harnessing the power of word embeddings, generated from Large Language Models. The following picture already shows all the magic:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/img/keybert.png" class="img-fluid figure-img"></p>
<figcaption>Source: https://maartengr.github.io/KeyBERT/</figcaption>
</figure>
</div>
</section>
<section id="bertopic-httpsmaartengr.github.iobertopicindex.html" class="level3">
<h3 class="anchored" data-anchor-id="bertopic-httpsmaartengr.github.iobertopicindex.html">BERTopic (https://maartengr.github.io/BERTopic/index.html)</h3>
<p>A more comprehensive approach, which also harnesses the power of word embeddings is called BERTopic. BERTopic is an approach that builds upon a NLP pipeline to extract topics for a corpus.</p>
<p>The different steps in the algorithm are: 1. Embedding of all documents in the same vector space 2. Reducing the dimensionality of the embeddings 3. Clustering the documents into groups of potentially the same topics 4. Building a bag of words of the previously computed clusters by concatenating all documents within a cluster 5. Using a weighted form of the TF-IDF algorithm (IDF focusses on clusters not documents) to extract relevant topics 6. Fine-tune the result (optional)</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/img/bertopic.png" class="img-fluid figure-img"></p>
<figcaption>Source: https://maartengr.github.io/BERTopic/index.html</figcaption>
</figure>
</div>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/github\.com\/juergenct\/edv_tek");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="./index.html" class="pagination-link" aria-label="Projekt EDV-TEK">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Projekt EDV-TEK</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© 2024 by <a href="./about.html">Institute of Entrepreneurship - Jürgen Thiesen</a><br> <a href="./imprint.html">Imprint</a> | <a href="./privacy.html">Data Privacy</a></p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/juergenct/edv_tek/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>Made with <a href="https://quarto.org/">Quarto</a><br> View the source at <a href="https://github.com/juergenct/edv_tek">GitHub</a></p>
</div>
  </div>
</footer>




</body></html>